<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion">
  <meta name="keywords" content="RDLA, Rolling Diffusion, Co-Speech Gestures, Gesture Generation, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RDLA: Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ“„</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Evgeniia Vu</a>,</span>
            <span class="author-block">
              <a href="">Andrei Boiarov</a>,</span>
            <span class="author-block">
              <a href="">Dmitry Vetrov</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Constructor University, Constructor Tech</span>
          </div>

          <div class="is-size-3 publication-authors">
            <span class="author-block" style="font-weight: bold;">AAAI 2026</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.10488"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/andrewbo29/co-speech-gestures-rolling-diffusion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>

          <!-- TL;DR -->
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <div class="content has-text-justified">
                <p>
                  <strong class="is-size-5">TL;DR:</strong>&nbsp;&nbsp;We introduce a mixed diffusion-autoregressive framework that transforms existing diffusion-based sequence generation models into real-time streaming methods, achieving a 4Ã— speedup and superior temporal coherence without sacrificing motion quality or diversity.
                </p>
              </div>
            </div>
          </div>
          <!--/ TL;DR -->

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video commented out
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
-->

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clip_1_Happy_rolling_r.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clip_4_Angry_rolling_r.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clip_3_Happy_rolling_r.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clip_5_Relaxed_rolling_r.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generating co-speech gestures in real time requires both
temporal coherence and efficient sampling. We introduce a
novel framework for streaming gesture generation that extends Rolling Diffusion models with structured progressive
noise scheduling, enabling seamless long-sequence motion
synthesis while preserving realism and diversity. Our framework is universally compatible with existing diffusion-based
gesture generation model, transforming them into streaming
methods capable of continuous generation without requiring
post-processing. We evaluate our framework on ZEGGS and
BEAT, strong benchmarks for real-world applicability. Applied to state-of-the-art baselines on both datasets, it consistently outperforms them, demonstrating its effectiveness as
a generalizable and efficient solution for real-time co-speech
gesture synthesis. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that employs a
ladder-based noise scheduling strategy to simultaneously denoise multiple frames. This significantly improves sampling
efficiency while maintaining motion consistency, achieving
up to a 4Ã— speedup with high visual fidelity and temporal coherence in our experiments. Comprehensive user studies further validate our frameworkâ€™s ability to generate realistic, diverse gestures closely synchronized with the audio input.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework</h2>
        <div class="content">
          <img src="./static/images/visualization.png" alt="Framework visualization" style="max-width: 80%;">
        </div>
        <div class="content has-text-justified">
          <p>
            In our work, we adapt rolling diffusion models for co-speech
gesture generation, introducing a novel framework that
transforms any diffusion-based architecture into a streaming
model. Our approach enables seamless and continuous gesture generation of arbitrary length by modifying the model
architecture and integrating a structured noise scheduling
mechanism, which, combined with the rolling denoising process, ensures smooth temporal transitions and prevents
abrupt motion discontinuities. The model generates a new clean frame in each s-step and shifts
the generation window forward to include the new frame at
the end.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Rolling Diffusion Ladder Acceleration -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Rolling Diffusion Ladder Acceleration</h2>
        <div class="content">
          <img src="./static/images/ladder_steps_s_1.png" alt="Rolling Diffusion Ladder Acceleration visualization" style="max-width: 80%;">
        </div>
        <div class="content has-text-justified">
          <p>
            We introduce Rolling Diffusion Ladder Acceleration
(RDLA), a novel approach that transforms the original noise
schedule into a ladder with step size l, enabling
the simultaneous denoising of l frames from the same noise level. This modification allows multiple frames to be jointly denoised in each iteration, accelerating the process.
By implementing RDLA, we achieve a substantial reduction in inference time while maintaining high visual
fidelity and temporal consistency. Empirical results demonstrate that RDLA accelerates gesture synthesis
by up to 4Ã— compared to standard rolling diffusion.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Quantitative Comparison -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Comparison</h2>
        <div class="content">
          <div class="columns is-centered">
            <div class="column">
              <img src="./static/images/zeggs_results.png" alt="ZEGGS results" style="max-width: 100%;">
            </div>
            <div class="column">
              <img src="./static/images/beat_results.ong.png" alt="BEAT results" style="max-width: 100%;">
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            To thoroughly examine the impact of our method, we integrate our progressive noise scheduling technique into multiple baseline models and conduct comparisons across two
datasets: ZEGGS and BEAT (Tables 1 and 2). As the primary baselines for our
work, we selected state-of-the-art diffusion-based models for gesture generation: Taming Diffusion, DiffuseStyleGesture (DSG), PersonaGestor and DiffSHEG. To evaluate the quality of our generated gestures, we utilize Frechet distance (FD) and  Diversity
(Div) metrics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- User Study -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">User Study</h2>
        <div class="content">
          <img src="./static/images/user_study_all_big_3.png" alt="User Study results" style="max-width: 100%;">
        </div>
        <div class="content has-text-justified">
          <p>
            To assess the quality of our generated co-speech gestures,
we conducted a user study using pairwise comparisons between our model and a baseline. We selected the ZEGGS
dataset for its clear and expressive gestures, which allow for
a precise evaluation of movement quality, stylistic consistency, and synchronization with speech. We used the DSG
model as a baseline for comparison.
          </p>
          <p>
             Our rolling modification of DSG significantly outperforms the original DSG, aligning well with the quantitative
evaluation results (Left chart). To compare
RDLA with our original method, we conducted a user study
between DSG rolling and RDLA with l = 2. The distribution of the RDLA user study results is shown
Right chart. The RDLA approach is only slightly inferior to our
original method, which is consistent with the quantitative
findings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Acceleration Findings -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acceleration Findings</h2>
        <div class="content">
          <img src="./static/images/acceleration_findings.png" alt="Acceleration Findings" style="max-width: 60%;">
        </div>
        <div class="content has-text-justified">
          <p>
            On a NVIDIA A40 (48 GB), our
rolling DSG and RDLA variants greatly reduce latency without sacrificing throughput, as summarized in Table 3.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Old sections commented out
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a framework enabling real-time gesture synthesis synchronized with speech. We extend Rolling Diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. Our method works with existing diffusion-based gesture models to enable continuous generation without post-processing requirements.
          </p>
          <p>
            We introduce <strong>Rolling Diffusion Ladder Acceleration (RDLA)</strong>, which uses a ladder-based noise scheduling strategy to simultaneously denoise multiple frames, achieving approximately 4x speedup. Testing on ZEGGS and BEAT benchmarks demonstrated superior performance over baseline methods. User studies confirmed the framework generates realistic, diverse gestures closely synchronized with the audio input.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>

        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>
-->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{vu2025streaming,
  title     = {Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion},
  author    = {Vu, Evgeniia and Boiarov, Andrei and Vetrov, Dmitry},
  booktitle = {AAAI},
  year      = {2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution 4.0 License</a>. Website template credit: <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
          </p>
          <!-- <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
